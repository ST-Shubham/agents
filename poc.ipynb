{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906e3d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Enum for LLMProvider Options\n",
    "from enum import Enum\n",
    "class LLMProviderOptions(Enum):\n",
    "    OPENAI = \"openai\"\n",
    "    GEMINI = \"gemini\"\n",
    "    ANTHROPIC = \"anthropic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9097dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Base Config which will be implemented by model specific configs\n",
    "from pydantic import BaseModel, ConfigDict\n",
    "from typing import Optional\n",
    "\n",
    "class BaseAgentConfig(BaseModel):\n",
    "    model_config = ConfigDict(extra=\"forbid\")\n",
    "    model: str\n",
    "    temperature: Optional[float] = None\n",
    "    max_tokens: Optional[int] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cd8043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provider Specific Config Extensions\n",
    "from typing import Dict, Any\n",
    "\n",
    "class OpenAIConfig(BaseAgentConfig):\n",
    "    top_p: Optional[float] = None\n",
    "    presence_penalty: Optional[float] = None\n",
    "    frequency_penalty: Optional[float] = None\n",
    "\n",
    "\n",
    "class GeminiConfig(BaseAgentConfig):\n",
    "    safety_settings: Optional[Dict[str, Any]] = None\n",
    "    top_k: Optional[int] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada09702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model for messages\n",
    "from typing import List, Literal\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class Message(BaseModel):\n",
    "    role: Literal[\"system\", \"user\", \"assistant\", \"tool\"]\n",
    "    content: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de958b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base Provider class (llm-provider)\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Generator, Dict, Any, List, Type\n",
    "\n",
    "class LLMProvider(ABC):\n",
    "    config_schema: Type[BaseAgentConfig]\n",
    "\n",
    "    def __init__(self, config: BaseAgentConfig, system_message: str):\n",
    "        self.config = config\n",
    "        self.system_message = system_message\n",
    "        self.history: List[Message] = []\n",
    "\n",
    "    # -------------------------\n",
    "    # History Management\n",
    "    # -------------------------\n",
    "    def add_user_message(self, content: str):\n",
    "        self.history.append(Message(role=\"user\", content=content))\n",
    "\n",
    "    def add_assistant_message(self, content: str):\n",
    "        self.history.append(Message(role=\"assistant\", content=content))\n",
    "\n",
    "    def rollback(self, steps: int = 1):\n",
    "        \"\"\"Go back N messages (excluding system).\"\"\"\n",
    "        if steps > 0:\n",
    "            self.history = self.history[:-steps]\n",
    "\n",
    "    def save_history(self) -> List[Message]:\n",
    "        return self.history.copy()\n",
    "\n",
    "    def compress_history(self, summarizer: \"LLMProvider\"):\n",
    "        \"\"\"Replace history with a summary.\"\"\"\n",
    "        summary = summarizer.generate(\n",
    "            \"Summarize the following conversation:\\n\"\n",
    "            + \"\\n\\n\".join((m.role + \": \" + m.content) for m in self.history)\n",
    "        )\n",
    "        self.history = [\n",
    "            Message(role=\"assistant\", content=summary)\n",
    "        ]\n",
    "    \n",
    "    # -------------------------\n",
    "    # Messages Management\n",
    "    # -------------------------\n",
    "    \n",
    "    # allow changing system message at runtime (still not persisted)\n",
    "    def set_system_message(self, msg: Optional[str]):\n",
    "        self._system_message = msg\n",
    "\n",
    "    # helpers to build provider payload (default behavior)\n",
    "    def _build_messages_payload(self) -> List[Dict[str, str]]:\n",
    "        \"\"\"\n",
    "        Default neutral representation: system injected as first message if present.\n",
    "        Child providers may override to fit their API.\n",
    "        \"\"\"\n",
    "        msgs = []\n",
    "        if self._system_message:\n",
    "            msgs.append({\"role\": \"system\", \"content\": self._system_message})\n",
    "        for m in self.history:\n",
    "            msgs.append({\"role\": m.role, \"content\": m.content})\n",
    "        return msgs\n",
    "\n",
    "    # -------------------------\n",
    "    # Generation APIs\n",
    "    # -------------------------\n",
    "    @abstractmethod\n",
    "    def generate(self, prompt: str, **kwargs) -> str:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def generate_stream(self, prompt: str, **kwargs) -> Generator[str, None, None]:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def generate_structured(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        response_schema: Dict[str, Any],\n",
    "        **kwargs\n",
    "    ) -> Dict[str, Any]:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def generate_with_tools(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        tools: List[BaseModel],\n",
    "        enforce_tool_use: bool = False,\n",
    "        stream: bool = False,\n",
    "        **kwargs\n",
    "    ):\n",
    "        pass\n",
    "\n",
    "    # -------------------------\n",
    "    # Introspection\n",
    "    # -------------------------\n",
    "    @classmethod\n",
    "    def supported_config(cls) -> Dict[str, Any]:\n",
    "        \"\"\"Expose valid config fields to user/dev.\"\"\"\n",
    "        return cls.config_schema.model_json_schema()\n",
    "    \n",
    "    @classmethod\n",
    "    @abstractmethod\n",
    "    def capability_metadata(cls) -> Dict[str, Any]:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70713d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenAIProvider(LLMProvider):\n",
    "    config_schema = OpenAIConfig\n",
    "\n",
    "    def generate(self, prompt: str, **kwargs) -> str:\n",
    "        self.add_user_message(prompt)\n",
    "        # OpenAI SDK call here\n",
    "        response = \"openai-response\"\n",
    "        self.add_assistant_message(response)\n",
    "        return response\n",
    "\n",
    "    def generate_stream(self, prompt: str, **kwargs):\n",
    "        yield \"openai-stream\"\n",
    "\n",
    "    def generate_structured(self, prompt, response_schema, **kwargs):\n",
    "        return {}\n",
    "\n",
    "    def generate_tool_calls(self, prompt, tools, stream=False, **kwargs):\n",
    "        return {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdf11e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeminiProvider(LLMProvider):\n",
    "    config_schema = GeminiConfig\n",
    "\n",
    "    def generate(self, prompt: str, **kwargs) -> str:\n",
    "        self.add_user_message(prompt)\n",
    "        response = \"gemini-response\"\n",
    "        self.add_assistant_message(response)\n",
    "        return response\n",
    "\n",
    "    def generate_stream(self, prompt: str, **kwargs):\n",
    "        yield \"gemini-stream\"\n",
    "\n",
    "    def generate_structured(self, prompt, response_schema, **kwargs):\n",
    "        return {}\n",
    "\n",
    "    def generate_tool_calls(self, prompt, tools, stream=False, **kwargs):\n",
    "        return {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ada796",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Type, Union\n",
    "\n",
    "PROVIDER_REGISTRY: Dict[LLMProviderOptions, Type[LLMProvider]] = {\n",
    "    LLMProviderOptions.OPENAI: OpenAIProvider,\n",
    "    LLMProviderOptions.GEMINI: GeminiProvider,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76d9cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_agent(\n",
    "    provider: LLMProviderOptions,\n",
    "    config: Union[OpenAIConfig, GeminiConfig],\n",
    "    system_message: str,\n",
    ") -> LLMProvider:\n",
    "    provider_cls = PROVIDER_REGISTRY[provider]\n",
    "\n",
    "    # Runtime safety: ensure correct config type\n",
    "    if not isinstance(config, provider_cls.config_schema):\n",
    "        raise TypeError(\n",
    "            f\"{provider.value} expects {provider_cls.config_schema.__name__}\"\n",
    "        )\n",
    "\n",
    "    return provider_cls(config=config, system_message=system_message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53688c3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
